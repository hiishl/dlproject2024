{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import urllib.request\n",
    "\n",
    "from gpt.char import CharDataset\n",
    "from gpt.config import Config\n",
    "from gpt.model import GPTModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config(block_size=128, batch_size=128, d_emb=768, n_heads=8, n_layers=12, drop_rate=0.1, d_mlp=4, qkv_bias=True, vocab_size=65)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "with urllib.request.urlopen(url) as response:\n",
    "   data = response.read().decode('utf-8')\n",
    "\n",
    "cfg = Config()\n",
    "cd = CharDataset(cfg, data)\n",
    "cfg.vocab_size = cd.get_vocab_size()\n",
    "cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(cd, batch_size=cfg.batch_size, shuffle=True)\n",
    "model = GPTModel(cfg)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(1):\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        # move data to device\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # forward pass\n",
    "        logits = model(x)  # (batch_size, block_size, vocab_size)\n",
    "        loss = criterion(logits, y)\n",
    "        # backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print progress\n",
    "        if batch_idx % 10 == 0:  # for every 10 batches\n",
    "            print(f\"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model after train\n",
    "torch.save(model, \"gpt_model_full.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and decoder\n",
    "def tokenize(seq):\n",
    "    return torch.tensor([cd.stoi.get(x) for x in seq])\n",
    "\n",
    "def decode(tokens):\n",
    "    return ''.join([cd.itos[x.item()] for x in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the saved model for inference\n",
    "model = GPTModel(cfg)\n",
    "model.load_state_dict(torch.load(\"gpt_model.pth\"))\n",
    "# set the model to evaluation mode: no grad\n",
    "model.eval()\n",
    "# disable gradient calculation during inference\n",
    "with torch.no_grad():\n",
    "    model.generate(???) # change the temperature setting or top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 65])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(cd.__getitem__(1)[0].view(1, -1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "Yo\n"
     ]
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "\n",
    "with urllib.request.urlopen(url) as response:\n",
    "   data = response.read().decode('utf-8')\n",
    "\n",
    "print('Total number of characters:', len(data))\n",
    "print(data[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits batches of characters.\n",
    "\n",
    "    Adapted from \"https://github.com/karpathy/minGPT\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, data):\n",
    "\n",
    "        chars = ... # get characters from the input data\n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) } # map characters to integer indices\n",
    "\n",
    "        ...\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        # encode every character to an integer\n",
    "        # return the chunk and the shifted version as tensors\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tockenization\n",
    "\n",
    "$$\n",
    "\\text{text} \\rightarrow \\text{chars} \\rightarrow \\text{tokens} \\rightarrow \\text{embedding vectors}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits batches of characters.\n",
    "\n",
    "    Adapted from \"https://github.com/karpathy/minGPT\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, data):\n",
    "        \n",
    "        chars = sorted(list(set(data))) # get characters from the input data; tokens\n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) } # map characters to integer indices; tokens to token IDs -- vocabulary\n",
    "        \n",
    "        self.itos = { i:ch for i,ch in enumerate(chars) } # map integer indices to characters; decoding\n",
    "\n",
    "        self.vocab_size = len(chars)\n",
    "        self.data_size = len(data)\n",
    "        self.data = data\n",
    "        self.block_size = config['block_size'] # number of tokens for each sequence\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_size - self.block_size #??\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        chunk = self.data[idx:idx+self.block_size+1]\n",
    "        # encode every character to an integer\n",
    "        encoded = torch.tensor([self.stoi[c] for c in chunk], dtype=torch.long)\n",
    "        # return the chunk and the shifted version as tensors\n",
    "        x = encoded[:-1] # contains the input tokens\n",
    "        y = encoded[1:] # contains the output tokens\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'block_size':128, 'batch_size':128}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['z'] -----> ['e']\n",
      "['z', 'e'] -----> ['n']\n",
      "['z', 'e', 'n'] -----> [':']\n",
      "['z', 'e', 'n', ':'] -----> ['\\n']\n",
      "['z', 'e', 'n', ':', '\\n'] -----> ['B']\n",
      "['z', 'e', 'n', ':', '\\n', 'B'] -----> ['e']\n",
      "['z', 'e', 'n', ':', '\\n', 'B', 'e'] -----> ['f']\n",
      "['z', 'e', 'n', ':', '\\n', 'B', 'e', 'f'] -----> ['o']\n",
      "['z', 'e', 'n', ':', '\\n', 'B', 'e', 'f', 'o'] -----> ['r']\n",
      "['z', 'e', 'n', ':', '\\n', 'B', 'e', 'f', 'o', 'r'] -----> ['e']\n"
     ]
    }
   ],
   "source": [
    "cd = CharDataset(config, data)\n",
    "x, y = cd.__getitem__(10)\n",
    "\n",
    "for i in range(10):\n",
    "    context = x[:10][:i+1]\n",
    "    context = [cd.itos[i] for i in context.tolist()]\n",
    "    desired = y[:10][i]\n",
    "    desired = [cd.itos[desired.tolist()]]\n",
    "    print(context, '----->', desired)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.3035,  ..., -0.3181, -1.3936,  0.5226],\n",
      "        [ 0.2579,  0.3420, -0.8168,  ..., -0.4098,  0.4978, -0.3721],\n",
      "        [ 0.7957,  0.5350,  0.9427,  ..., -1.0749,  0.0955, -1.4138],\n",
      "        ...,\n",
      "        [-0.1837,  1.1975,  1.1828,  ...,  2.0874, -1.2495,  0.4475],\n",
      "        [ 1.3652,  0.1446, -1.2063,  ..., -3.0424, -1.5444,  1.4202],\n",
      "        [ 0.5061, -1.6644,  0.5144,  ..., -0.0105,  0.2888,  1.6012]],\n",
      "       requires_grad=True)\n",
      "torch.Size([65, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dim_embd = 768\n",
    "embedding_layer = torch.nn.Embedding(cd.get_vocab_size(), dim_embd)\n",
    "print(embedding_layer.weight)\n",
    "print(embedding_layer.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 768])\n",
      "torch.Size([128, 768])\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(x).shape)\n",
    "print(embedding_layer(y).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# position\n",
    "pos_embedding_layer = torch.nn.Embedding(config['block_size'], )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Multi-head Self-attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional encodings, residual connections, layer normalizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
